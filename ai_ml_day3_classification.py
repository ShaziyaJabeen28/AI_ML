# -*- coding: utf-8 -*-
"""AI/ML_Day3_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ECypKoLUTtcKzBtQ7VJZ59Yl1EwSHh9O
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('https://raw.githubusercontent.com/tech4alltraining/aiml/refs/heads/main/datasets/classification/iris.csv')
df
#species is the target variable

df.head()

df.tail()

df.info()

df.describe()

#Data Preprocessing check
#check Null values
#check duplicates
# check outliers
# Check whether  scaling required or not
#Check Label Encoding required or not

#Allow duplicates
# No outliers
#No missing values
# No scaling required
#Label Encoding required

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['species'] = le.fit_transform(df['species'])
df

from sklearn.model_selection import train_test_split
X = df.drop('species', axis=1)
y = df['species']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 42)

print("Shape of X_train: ", X_train.shape)
print("Shape of X_test: ", X_test.shape)
print("Shape of y_train: ", y_train.shape)
print("Shape of y_test: ", y_test.shape)

# Model Selection Training -
# 1.Logistic Regression
from sklearn.linear_model import LogisticRegression  #class  import #Logistic Regression is the classification algorithm It's commonly used for binary classification beacuse it is used to t
lr = LogisticRegression() #object / instance creation
lr.fit(X_train, y_train) # Training

y_test

y_pred = lr.predict(X_test) #Evaluation
y_pred

#Classification Evaluating matic
#1.Accuracy
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, y_pred))

#confusion matrix
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# prompt: plot confusion matrix

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# 2. kNN ALgorithm:
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5) #parameter means consider 5 neighbour (Values can be any) #this parameter is called hyperparameter
knn.fit(X_train, y_train)

#Test Model (Evaluation)
y_pred = knn.predict(X_test)
y_pred

#Accucary, COnfusion Matrix, Classification
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 3. Decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)

# testing
y_pred = dtc.predict(X_test)

# Accuracy, Confusion Matrix Classification Report
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 4. SVM
from sklearn.svm import SVC
svc = SVC(kernel='linear')
svc.fit(X_train, y_train)

y_pred = svc.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 5 . Naive Baye's Classifers
# Model GNB
from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(X_train, y_train)

y_pred = nb.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# 6. Random Forest
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=200)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.neural_network import MLPClassifier
#Create an MLP classifier
mlp = MLPClassifier(hidden_layer_sizes=(10,10,5), max_iter=1000, random_state=42)#hidden layer no of neuron in 1st layer 10 and so on

# Train the MLP model
mlp.fit(X_train, y_train)

# Make predictions on the test set
y_pred = mlp.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

